#+TITLE: Books

* SQL Antipatterns Volume 1
Avoiding the Pitfalls of Database Programming

** Chapter 1 - What is an anti pattern
** Chapter 2 - Jaywalking
Use comma separated lists or similar to avoid creating an intersection table for many-to-many relationships

Objective: Store multivalue attributes

Problems addressed in book:
- Querying with regexes or similar
- Aggregations like counts or similar have to do complicated string operations (length(replace(...)))
- Updating the list needs something like string concatenation and easy to mess up or have no order in the list, double entries and so on
- Removing is even worse
- No referential integrity for items in the list
- Separator character might be part of data (or if not it might be some time later)
- List length limitations

Further problems IMHO:
- Memory/Disk consumption in case of eg categorical textual list entries
- Even for exports, we'll still need custom logic to handle the lists for further processing most of the time
- Given a list, it's always unclear whether the order matters or not or if duplicates or intended or not
- It's easy to misinterpret the list structure as a scalar if most of the time the list consist of <= 1 items
- To join the items with other tables back, the rows need to be exploded,
  what is complex, seldom used, error prone (danger of duplications) and increases memory in a way no optimizer can work with well
  Especially in distributed computing, this can result in OOM, instance crashes and so on
  Databases are optimized for joins of normalized tables

Legitimate usages addressed in book:
- In some cases useful for denormalization (mostly readonly exports)
- Vendor specific SQL array types are a bit better, but still overcomplicate to use and need special syntax and still no integrity

Legitimate usages IMHO:
- Using documents content eg lists as json (binary) data resolves many issues from above and is supported relatively equally in all languages,
  but still no referential integrity and semantic interpretation and memory/storage is still uncontrolled and easily out of bounds
  Younger developer tend to overuse it for these reasons
- When working a lot with cli/chainable tools such list structures can be nice as export format (as long as it is not stored in this way internally)

Solution:
- Create intersection table
** Chapter 3 - Naive Trees
Table referencing tree structure as adjacency list via a parent foreign key (parent_id) or similar

Objective: Store and query hierarchies

Problems addressed in this book:
- Querying a tree with usual SQL gets awkward
  Needs many left outer joins for every hierarchical level and still handles only fixed number of levels
  Also can be problematic for performance and memory consumption
- Hard to compute aggregates like simple counts or filtering out sub levels on given conditions

Further problems IMHO:
- Hard to visualize (SQL databases and frontends usually aren't graph database or views)
- Not useful ad hoc for any Machine Learning, Feature Generation or easy Denormalization (e.g. to feed into an analytics engine or a standard ML approach like decision trees or clustering algos)
- Tough to do plausbilizations, data drift checks or other data tests or reporting

Legitimate usages addressed in book:
- still simple (KISS) format of most hierarchical data
- don't need support for unlimited depth
- if mainly operations are inserting/updating the information, not querying them

Legitimate usages IMHO:
- if it's getting forwarded to a Graph model, database or visualization anyway

Solution:
- Recursive Queries
  Disadvantages: steep learning curve and a bit unusual, and still performance, and a bit custom db specific)
- Path enumeration similar to unix file system ("1/4/5")
  Disadvantages: Jaywalking
- Nested sets where each node receives a nsleft <= all node nrs below in the hierarchy <= nsright
  Querying then is very fast, insertions are not fast
- Closure table having another Table referencing each node 1 : m all children including the node itself
  Fast querying, fast updating, deletion, inserting, but needs a lot of space
** Chapter 4 - ID Required
Add new column like `id` as pseudo key or surrogate key to every table so that every row can be addressed uniquely
while also allowing all other attributes to contain duplicates

Objective: Establish primary key conventions

Problems addressed in this book:
- cargo cult that every table "needs" a single (maybe even autogenerated) primary key all looking the same and maybe all called `id`
- might make a redundant key
- allows duplicate rows
- obscures the meaning of a key
  - `id` is so generic that it holds no meaning
  - especially if query result is returned as dict (like in JSON) where multiple `id` would just silently overwrite
- writing joins won't work with USING `id` and joins get ugly and worse error prone to be written
- avoiding compound keys is just wrong:
  - like mathematician refusing to use 2d or 3d coordinates

Further problems IMHO:
- such ids are easy to misinterpret:
  - confound with other ids
  - think their ordering might matter
  - if there is a natural key column, it might be called similar and confuses the hell out of everyone
- makes it tough to migrate or roll back changes from tests, other branches, run distributed, etc
- uses extra space
- doesn't have a meaning especially not for the subject matter experts, but also not for the database experts per se
- messes up any hashing, e.g. inserting, deleting and inserting again should yield the same table,
  but with such an autogenerated surrogate key, it won't be the same row and if you track via a hash,
  it might even receive a different one (or you need complicate logic to find out whether the id is a real one or a surrogate one)
- similar: it's completely unclear whether the id is important in case of exports or even in contrast security problematic
- can't use it to feed into a machine learning (it's even worse and ML could be find a meaningless correlation and overfit)

Legitimate usages:
- pseudo key is a good choice as a surrogate for a natural key that's too long to be practical
- data warehousing

Solution as in book:
- tell it like it is ::
  - sensible names for your primary key, like `bug_id` instead of `id`
  - *same* column name in foreign keys where possible (so joining via USING bug_id or similar will work like a charm)
  - name of primary key should be unique within your schema
- be unconventional :: change defaults for ORMs in case
- embrace natural keys and compound keys ::
  - if table contains an attribute that's guaranteed to be unique, non null and serve to identify the row => use it as primary key
  - use compound keys when they're appropriate

Further solutions IMHO:
- Hash the natural key candidates:
  - if it is unsure whether natural keys are guaranteed to stay natural keys in future
  - are too long
  - or have other problems, like they are sensitive and might not be exported or something like that, but you still need them for linking
** Chapter 5 - Keyless Entry
leaving out referential integrity to simplify database design/architecture/programming
instead of fail early whenever a user submitted invalid data

Problems addressed in this book:
- assuming flawless code (in application logic) :: "One in a million is next Tuesday" (regarding low probabilities of errors)
- checking for mistakes :: via own written scripts having to run regulary (something I had to do a lot when working with data)
  is more effort than just setting the right constraints and rely on the database to do it
- "it's not my fault" :: your application might be fine, but other applications or database users or ad hoc SQL might not
  and again this will happen at some point in the lifetime of an application (IHMO, especially if it a successful application)
- Catch-22 updates :: to make updates and keep referential integrities, you need more than one database SQL statement
  shutting off foreign key enables it, but for the price of having inconsistent views in between

Further problems IMHO:
- makes it very hard to work with database exports or dumps
  as the important relationship between the data gets lost (and documentation in practice either is not accessible or outdated)
  I had to write more than once scripts to reverse engineer the foreign key relationships
  what is extremely difficult if the key ranges overlap and/or there are already inconsistencies in the database (as a result of no foreign key design)
  so while RDBMS should be very strict math oriented set theory things -> they become a non scientific guessing game
- it's difficulty to check data pipelines on consistency or subtle errors if the constraints are not hard enforced

Legitimate usages addressed in book:
- when doing huge data cleanup projects
- renaming database tables can be become nasty with foreign key definitions
  (author mentions pt-online-schema-change tool for MySQL)
  IMHO I'm sceptic about and personally would try to delegate the migration to a good tool (like Django ORM migrations or w/e)

Legitimate usages IMHO:
- when building highly dynamic ELT pipelines
  it might be easier and more performant to avoid creating foreign key relations
  Note: It's important not to do any modifications (that's why it's ok for ELT, but IMHO not for ETL)
  I think it's especially right, if the destination database is one that only exists for short time,
  like if you bring it into a pandas/SQLite in memory (read only) presentation

Solution:
- Declare constraints
- Define cascading updates
** Chapter 6 - Entity-Attribute-Value
support variable attribute via a 2nd table with columns like (id, attr_name, attr_value)
also calls open schema, schemaless, name-value pairs
with:
- both tables have few columns
- number of columns doesn't need to grow to support new attributes
- avoid clutter of columns with many nulls

Problems addressed in this book:
- Querying :: will also need to use a WHERE attr_name = ? clause instead of a SELECT ..., attr_name statement
  IMHO I don't think that's so terrible, but more problematic is that slight typos or similar might be completely missed
- Data Integrity ::
  - can't make mandatory attributes
  - can't use SQL data types (usually attr_value will be just a string)
    IMHO in addition no really good and performant way to do even more than SQL types (like writing a constraint),
    even if you can do it, it will have a lot of branching inside, what's just terrible for readability, correctness, performance, exportability, maintainability
  - can't enforce referential integrity
  - can't make up attribute names like =attr_name IN ('date_reported', 'report_date'))=
    IMHO: this is a very common problem
- reconstructing a row is problematic (in pure SQL it's really ugly)
  IMHO: usually there are custom wide <> short format converters that make it manageable

Further problems:
- querying on a combination of different attr_name:attr_value pairs very easily becomes a mess,
  it's usually possible with aggregation functions, but ugly to write, ugly to read and can have many surprises if there are attr_name non unique (e.g. as multiple ids were selected etc)
- it's much tougher to make plausibility checks against possibly dirty data, e.g. typos in values, just as the domain for values is so much bigger and heterogenous
- same values might have complete different meanings (e.g. "true")
  that's confusing for us humans
  bad for quality checks
  bad for any easy to gather statistics
  terrible for any ML input
- it's even possible that same attr_name have a different meaning (maybe because they are coming from a different source)
- it's also possible that multiple same attr_name are mentioned (what's then the correct value or should it be multivalued?)
  that's manageable by setting a unique constraint, but again, that might influence performance and get lost in exports and is in every case possibly confusing for humans, but also for JSON exports or other such structures
- explaining this format to SME is no fun and handling by them has a huge potential to miss the points even if they know SQL well
  key value structures might be natural for programmers, but for the rest of the world flat, long tables are much more intuitive (with the exception of visualizing individual results in a frontend)

Legimate usages addressed in the book:
- hard to justify in a relational databases => if there are nonrelational data needs -> use NoSQL technology

Legitimate usages IMHO:
- if close to all data is relational, but you need one such "open schema" and going JSON or similar is no way (because tools don't really support it, let's say Excel likish ones)
  it's certainly better than to add a complete different tool (NoSQL, special logic to have JSON <> wide views, automatic converters or plugins)
- really: if most attributes are null most of the time, but there are many hundreds or even more of them
  IMHO you easily get lost in those many columns (it's not a problem for the machine or the SQL database, but for us humans)
  in case: at least try to nail down the attr_name by having them be a foreign key for attr_name lookup table (where you could also store things like comments, synonyms or differentiate between homonyms)

Solutions addressed in book:
- Single Table Inheritance ::
  - store all related types in one table
  - use one attribute to define the subtype of a given row
  - best when only few subtypes and few subtype specific attributes and a need for a single table access pattern
- Concrete Table Inheritance ::
  - create separate table for each subtype
  - every table contains same attributes that are common in the base type plus the specific subtype attributes
  - best if you seldom need to query agains all subtypes at once
- Class Table Inheritance ::
  - single table for the base type
  - for each sub type create another table with a primary key that is also a foreign key to the base table
  - best if you need often to query across all subtypes, referencing the columns they have in common
- Semistructured Data ::
  - use something like JSON or JSONB column
  - it's for at least completely extensible and more or less standard format with tools for it (custom SQL support, but also things like JSON schema, etc)
  - best used, when you can't limit yourself to a finite set of subtypes and need complete flexibility to define new attributes at any time
- Post-Processing ::
  - don't try to write queries that fetch entities as a single row as though data were stored in a conventional table
  - just query all rows for specific ids
  - write application code to loop over it
  IMHO:
  - works only for small data, but then very well, especially with tools like pandas, dplyr, ...
  - filtering down ids for specific keys might be necessary anyway (possible, but not very performant with subqueries)
  - if you can arrange data to be local structure (partition by id e.g.), pivoting locally and transforming into a wide table format is possible
    (again, might not be very performant depending on the database)
    but leads to having natural queries instead of many tedious joins and/or subqueries

Further Solutions IMHO:
- it's possible to write triggers in PL/SQL or other languages supported by the database that
  automatically create a read only copy of a key value table (and all inserts, updates, deletes) into wide format table
  (probably adding columns ad hoc if needed)
  it's redundant and has a suprise and little performance penalty on changes (but is fast to read then), and works ok in practice
  biggest disadvantage is that bigger refactorings can become a mess
  if you need this demand on multiple occassions, it might be worth to do a real CQRS solution with a streaming tool like Kafka
  a subtle disadvantage is that it is hard to check both tables keep in sync (or to supervision the copy trigger logic)
 - Note: You should not design a database to work with such a schema intentionally
         That's a workaround if the input data is in key value format and SMEs etc are used to work with it to some degree,
         but so you have to keep it in such a raw format, but still would want to write proper SQL for further work.
** Chapter 7 - Polymorphic Associations
Reference multiple different parents for a common artefact table (like comments, blogs, images, ...)

Anti-Pattern: Use dual purpose foreign key
so you reference the artefact_id with joining on them, but without having foreign key relations (because you can only have a foreign key to one of the parents, not to all)

Problems addressed in book:
- Referential integrity is not enforced by database
- can only reference parents by left outer join them, resulting in duplicate column names if parents have columns in common (not a problem for SQL but for humans)
  or by having a parent_type information that again is not enforced by database

Legitimate usages addressed in book:
- ORMs might do something like that for us (and that might be ok, as it should battle proofed), but don't do such designs from scratch

Solutions:
- Creating intersection tables ::
  #+BEGIN_SRC sql
---- Comments
     ---- BugsComments    ---- Bugs
     ---- FeatureComments ---- Features

CREATE TABLE BugsComments(  -- and similar FeatureComments
       issue_id     BIGINT UNSIGNED NOT NULL,
       comment_id   BIGINT UNSIGNED NOT NULL,
       UNIQUE KEY (comment_id),  -- if a comment shouldn't be used in different bugs, still would allow to have a comment used in a Bug and in a Feature (needs application logic to constraint that)
       PRIMARY KEY (issue_id, comment_id),
       FOREIGN KEY (issue_id) REFERENCES Bugs(issue_id),  -- or Feature(issue_id)
       FOREIGN KEY (comment_id) REFERENCES Comments(comment_id)
);
  #+END_SRC
- Create common super table ::
  #+BEGIN_SRC sql
---- Issues
     ---- Bugs
     ---- Features
     ---- Comments

CREATE TABLE Issues(
       issue_id     SERIAL PRIMARY KEY
);
CREATE TABLE Bugs( -- and similar Features
       issue_id     BIGINT UNSIGNED PRIMARY KEY,
       FOREIGN KEY (issue_id) REFERENCES Issues(issue_id),
       ...
);
CREATE TABLE Comments(
       comment_id   SERIAL PRIMARY KEY,
       issue_id     BIGINT UNSIGNED NOT NULL,
       ...,  -- author, comment text, date, ...
       FOREIGN KEY (issue_id) REFERENCES Issues(issue_id)
);
  #+END_SRC


* Data Pipelines with Apache Airflow
** Chapter 1 - Meet Apache Airflow
- Data pipelines as graphs ::
  works on DAGs, so does not contain any loops or cycles
  extremely important, as it prevents us from running into circular dependencies
- Pipeline graphs vs sequential scripts ::
  single monolithic script may not initially seem like that much of a problem,
  but it can introduce some inefficiencies when tasks in the pipeline fail
- Defining pipelines flexibility in (python) code ::
  in airflow, define you DAGs using Python code in DAG files,
  which are essentially Python scripts that describe the structure of the corresponding DAG
- Reasons to choose Airflow ::
  features such as backfilling enable to easily (re)process historical data,
  allowing to recompute any derived data sets after making changes to your code
- Reasons not to choose Airflow ::
  - handling streaming pipelines
  - implementing highly dynamic pipelines
    although Airflow can implement this kind of dynamic behaviour,
    the web interface will only show tasks that are still defined in the most recent version of the DAG
  - it's primarly a workflow/pipeline management platform,
    does not contain features as maintaining data lineages, data versioning, ...
- Summary ::
  implementing efficient, batch-oriented data pipelines
** Chapter 2 - Anatomy of an Airflow DAG
- Running Airflow in a Python environment ::
  Make sure to install apache-airflow and not just airflow
- Summary ::
  - Workflows in Airflow are represented as DAGs
  - Operators represent a single unit of work
  - Airflow contains an array of operators both for generic and specific types of work
  - Airflow UI offers a graph view for viewing the DAG structure and tree view for viewing DAG runs over time
  - Failed tasks can be restarted anywhere in the DAG
** Chapter 3 - Scheduling in Airflow
- Defining scheduling intervals :: e.g.
#+BEGIN_SRC python
dag = DAG(
    dag_id="02_daily_schedule",
    schedule_interval="@daily",
    start_date=dt.datetime(2023, 1, 1),
    ...
)
#+END_SRC
  Airflow starts tasks in an interval *at the end of the interval*
  so @daily will run at end of day at midnight
- Cron based intervals :: min hour day_of_the_month month day_of_week_sunday_to_saturday
- Frequently used scheduling intervals ::
  - @once
  - @hourly
  - @daily
  - @weekly
  - @monthly
  - @yearly
- Frequency based intervals :: e.g.
  #+BEGIN_SRC python
dag = DAG(
    dag_id="02_daily_schedule",
    schedule_interval=dt.timedelta(days=3),
    start_date=dt.datetime(2023, 1, 1),
    ...
)
  #+END_SRC
- Dynamic time references using execution dates ::
  via context variables:
  - execution_date :: datetime of start of current execution interval
  - next_execution_date :: datetime of end of current execution interval
  - previous_execution_date :: start of previous execution interval

  can be used e.g. via jinja2 templating like
    #+BEGIN_SRC python
  fetch_events = BashOperator(
      task_id="fetch_events",
      bash_command=f"""
         ... &&
         curl ...?start_date={{execution_date.strftime("%Y-%m-%d")}}&end_date={{next_execution_date.strftime("%Y-%m-%d")}}
      """
  )
    #+END_SRC

    shorthand notations:
    - ds :: execution_date.strftime("%Y-%m-%d")
    - next_ds, next_ds_nodash, prev_ds, prev_ds_nodash :: similar
- partitioning data :: possible via a templates_dict in context variable, e.g.
  #+BEGIN_SRC python
def calculate_some_stats(**context):
    input_path = context["templates_dict"]["input_path"]
    output_path = context["templates_dict"]["output_path"]

    ...

calculate_stats_operator = PythonOperator(
    task_id="calculate_stats",
    python_callable=calculate_some_stats,
    templates_dict={
        "input_path": "/data/events/{{ds}}.json",
        "output_path": "/data/stats/{{ds}}.csv"
    },
    dag=dag
)
  #+END_SRC
- Understanding Airflow's execution dates ::
  interval-based approach has advantage that it is exactly known for which time interval the task has to work in contrast to cron jobs

  *Caveat*: interval parameters can be undefined if runs are triggered manually in Airflow
- Using backfilling to fill in past gaps ::
  by default, Airflow will schedule and run *any past* schedule that have not been run
  so will result in all intervals that have been passed before the current time being executed

  controlled by =catchup= parameter, e.g.:
  #+BEGIN_SRC python
dag = DAG(
    dag_id="09_no_catchup",
    schedule_interval="@daily",
    start_date=dt.datetime(year=2023, month=1, day=1),
    end_date=dt.datetime(year=2024, month=1, day=1),
    catchup=False,  # don't rerun dayly tasks for previous days than current day in case
)
  #+END_SRC

  can be used to reprocess data after we've made changes in our code
- Best practices for designing tasks ::
  Airflow tasks: atomicity and idempotency
** Chapter 4 - Templating tasks using the Airflow context
- Templating operator arguments :: Airflow uses pendulum library for datetime
- What is available for templating ::
  Context variables
  |---------------------------+---------------------------------------------------------------------------------------------------|
  | conf                      | Airflow configuration                                                                             |
  | dag                       | current DAG object                                                                                |
  | dag_run                   | current DagRun object                                                                             |
  | ds                        | execution_date formatted as %Y-%m-%d                                                              |
  | ds_nodash                 | execution_date formatted as %Y%m%d                                                                |
  | execution_date            | start datetime of current interval                                                                |
  | inlets                    | ??                                                                                                |
  | macros                    | ??                                                                                                |
  | next_ds                   | execution date of next interval (=end of current interval) formatted as %Y-%m-%d                  |
  | next_ds_no_dash           | execution date of next interval (=end of current interval) formatted as %Y%m%d                    |
  | next_execution_date       | execution date of next interval (=end of current interval)                                        |
  | outlets                   | ??                                                                                                |
  | params                    | user provided variables to the task context (intended for key value pairs dynamically configured) |
  | prev_ds                   | execution date of previous interval formatted as %Y-%m-%d                                         |
  | prev_ds_nodash            | execution date of previous interval formatted as %Y%m%d                                           |
  | prev_execution_date       | execution date of previous interval                                                               |
  | prev_start_date_success   | date and time of which the last successful run of the same task (only in past) was started        |
  | run_id                    | DagRun's run_id                                                                                   |
  | task                      | current operator                                                                                  |
  | task_instance (short: ti) | current TaskInstance object                                                                       |
  | task_instance_key_str     | unique identifier for current TaskInstance ={dag_id}_{task_id}_{ds_nodash}=                       |
  | templates_dict            | user provided variables to the task context (intended for default variables)                      |
  | test_mode                 | boolean whether airflow is running in test model                                                  |
  | tomorrow_ds               | ds plus one day                                                                                   |
  | tomorrow_ds_nodash        | ds_nodash plus one day                                                                            |
  | ts                        | execution_date formatted as ISO8601                                                               |
  | ts_nodash                 | execution_date formatted as ISO8601 without dashes                                                |
  | ts_nodash_with_tz         | execution_date formatted as ISO8601 without dashes but with timezones                             |
  | var                       | helpers objects for dealing with Airflow variables                                                |
  | yesterday_ds              | ds minus one day                                                                                  |
  | yesterday_ds_nodash       | ds_nodash minus one day                                                                           |
  |---------------------------+---------------------------------------------------------------------------------------------------|
- Templating to PythonOperator ::
  is an exception to above, needs instead a python_callable that explicitly demands the needed variables

  #+BEGIN_SRC python
dag = DAG(...)
def get_data(execution_date):
    ...

get_data_operator = PythonOperator(
    task_id="get_data",
    python_callable=get_data,
    dag=dag
)
  #+END_SRC

  or

  #+BEGIN_SRC python
def get_data(**context):
    start = context["execution_date"]
    ...
  #+END_SRC

  or even

    #+BEGIN_SRC python
def get_data(execution_date, **context):
    start = execution_date
    remaining_context = context
    ...
  #+END_SRC
- Providing variables to the PythonOperator ::
  #+BEGIN_SRC python
get_data_operator = PythonOperator(
    task_id="get_data",
    python_callable=get_data,
    op_args=["arg1", "arg2", ...],
    op_kwargs={"keyX": "argX", "keyY": "argY", ...},
)
  #+END_SRC
- Inspecting templated arguments ::
  in UI via clicking the Rendere Template button after running a task

  by CLI via =airflow task render [dag_id] [task_id] [desired execution date]
- Hooking up other systems ::
  passing data between taks:
  - either using Airflow metastore to read and write results between Tasks => *XCom*
    - only suitable for smaller objects
    - typically advised to apply XComs only for transferring small pieces of data such as a handful of strings
  - or by writing results to and from a persistent location
    - ususally via one of the providers (with pip packages like =apache-airflow-providers-*=, eg =apache-airflow-providers-postgres=)
    - might instantiate a hook (dealing with creating connections, sending queries and closing connections again)
    - operators determine what has to be done, hooks determine how to do something
    - ususally when building pipelines, you'll only deal with operators: hooks are used internally in operators
** Chapter 5 - Defining dependencies between tasks
- Linear dependencies :: via >> operator like
  #+BEGIN_SRC python
# either separate as
download_launches >> get_pictures
get_pictures >> notify

# or multiple dependencies in one go
download_launches >> get_pictures >> notify
  #+END_SRC
#+END_SRC
- Fan in/out dependencies ::
  #+BEGIN_SRC python
from airflow.operators.dummy import DummyOperator

start = DummyOperator(task_id="start")  # dummy start task
start >> [fetch_wheather, fetch_sales]  # fan out

fetch_wheather >> clean_wheather  # linear dependencies that run in parallel
fetch_sales >> clean_sales

[clean_wheather, clean_sales] >> join_datasets  # fan in

join_datasets >> train_model >> deploy_model  # just simple linear dependencies
  #+END_SRC
- Branching within tasks :: flexible, but difficult to see which code branch is being used
  #+BEGIN_SRC python
def _clean_sales(**context):
    if context["execution_date"] < ERP_CHANGE_DATE:
        _clean_sales_old(**context)
    else:
        _clean_sales_new(**context)
  #+END_SRC
- Branching within DAG :: =BranchPythonOperator= expected to return ID of downstream task
  #+BEGIN_SRC python
fetch_sales_old = PythonOperator(...)
clean_sales_old = PythonOperator(...)

fetch_sales_new = PythonOperator(...)
clean_sales_new = PythonOperator(...)

fetch_sales_old >> clean_sales_old
fetch_sales_new >> clean_sales_new

pick_erp_system = BranchPythonOperator(  # <-- Explicit branching
    task_id="pick_erp_system",
    python_callable=_pick_erp_system,  # depending on day or w/w will either return "fetch_sales_old" or "fetch_sales_new"
)

join_datasets = PythonOperator(
    ...,
    trigger_rule="none_failed"  # <-- now only one of two branches will succeed, so need to adapt the triggering
)
  #+END_SRC
- Conditions within tasks :: again flexible, but invisible
  #+BEGIN_SRC python
def _deploy_conditionally(**context):
    if context["execution_date"] == ...:
        deploy_model()

deploy_if_most_recent_run = PythonOperator(
    task_id="deploy_model_if_most_recent_run",
    python_callable=_deploy_conditionally
)
  #+END_SRC
- Making tasks conditional :: adding task that raises =AirflowSkipException= if downstream tasks should be skipped
  #+BEGIN_SRC python
from airflow.exceptions import AirflowSkipException

def _latest_only(**context):
    left_window = context["dag"].following_schedule(context["execution_date"])
    right_window = context["dag"].following_schedule(left_window)

    now = pendulum.utcnow()
    if not left_window < now <= right_window:
        raise AirflowSkipException("Not the most recent run")

latest_only = PythonOperator(
    task_id="latest_only",
    python_callable=_latest_only,
    dag=dag
)

latest_only >> deploy_model
  #+END_SRC
- Using builtin operators :: =LatestOnlyOperator=
  #+BEGIN_SRC python
from airflow.operators import LatestOnlyOperator

latest_only = LatestOnlyOperator(
    task_id="latest_only",
    dag=dag
)

train_model >> latest_only >> deploy_model
  #+END_SRC
- Trigger rules ::
  |--------------+----------------------------------------------------+----------------------------------------------------------------------------------|
  | all_success  | all parent tasks have completed successful         | default                                                                          |
  | all_failed   | all parents (or their ancestors) have failed       | to trigger error handling when at least one should be succeeded                  |
  | all_done     | all parents are done independent of success        | to execute clean up code                                                         |
  | one_failed   | trigger as soon as at least one parent failed      | quickly trigger some error handling like notifications or rollbacks              |
  | one_success  | trigger as soon as at least one parent succeeded   | quickly trigger downstream computations/notifications as one result is available |
  | none_failed  | all parents completed or were skipped              | for conditional branching                                                        |
  | none_skipped | no parents have been skipped                       | trigger code if all upstream tasks were executed                                 |
  | dummy        | triggers regardless of state of any upstream tasks | testing                                                                          |
  |--------------+----------------------------------------------------+----------------------------------------------------------------------------------|
