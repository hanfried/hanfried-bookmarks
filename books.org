#+TITLE: Books

* SQL Antipatterns Volume 1
Avoiding the Pitfalls of Database Programming

** Chapter 1 - What is an anti pattern
** Chapter 2 - Jaywalking
Use comma separated lists or similar to avoid creating an intersection table for many-to-many relationships

Objective: Store multivalue attributes

Problems addressed in book:
- Querying with regexes or similar
- Aggregations like counts or similar have to do complicated string operations (length(replace(...)))
- Updating the list needs something like string concatenation and easy to mess up or have no order in the list, double entries and so on
- Removing is even worse
- No referential integrity for items in the list
- Separator character might be part of data (or if not it might be some time later)
- List length limitations

Further problems IMHO:
- Memory/Disk consumption in case of eg categorical textual list entries
- Even for exports, we'll still need custom logic to handle the lists for further processing most of the time
- Given a list, it's always unclear whether the order matters or not or if duplicates or intended or not
- It's easy to misinterpret the list structure as a scalar if most of the time the list consist of <= 1 items
- To join the items with other tables back, the rows need to be exploded,
  what is complex, seldom used, error prone (danger of duplications) and increases memory in a way no optimizer can work with well
  Especially in distributed computing, this can result in OOM, instance crashes and so on
  Databases are optimized for joins of normalized tables

Legitimate usages addressed in book:
- In some cases useful for denormalization (mostly readonly exports)
- Vendor specific SQL array types are a bit better, but still overcomplicate to use and need special syntax and still no integrity

Legitimate usages IMHO:
- Using documents content eg lists as json (binary) data resolves many issues from above and is supported relatively equally in all languages,
  but still no referential integrity and semantic interpretation and memory/storage is still uncontrolled and easily out of bounds
  Younger developer tend to overuse it for these reasons
- When working a lot with cli/chainable tools such list structures can be nice as export format (as long as it is not stored in this way internally)

Solution:
- Create intersection table
** Chapter 3 - Naive Trees
Table referencing tree structure as adjacency list via a parent foreign key (parent_id) or similar

Objective: Store and query hierarchies

Problems addressed in this book:
- Querying a tree with usual SQL gets awkward
  Needs many left outer joins for every hierarchical level and still handles only fixed number of levels
  Also can be problematic for performance and memory consumption
- Hard to compute aggregates like simple counts or filtering out sub levels on given conditions

Further problems IMHO:
- Hard to visualize (SQL databases and frontends usually aren't graph database or views)
- Not useful ad hoc for any Machine Learning, Feature Generation or easy Denormalization (e.g. to feed into an analytics engine or a standard ML approach like decision trees or clustering algos)
- Tough to do plausbilizations, data drift checks or other data tests or reporting

Legitimate usages addressed in book:
- still simple (KISS) format of most hierarchical data
- don't need support for unlimited depth
- if mainly operations are inserting/updating the information, not querying them

Legitimate usages IMHO:
- if it's getting forwarded to a Graph model, database or visualization anyway

Solution:
- Recursive Queries
  Disadvantages: steep learning curve and a bit unusual, and still performance, and a bit custom db specific)
- Path enumeration similar to unix file system ("1/4/5")
  Disadvantages: Jaywalking
- Nested sets where each node receives a nsleft <= all node nrs below in the hierarchy <= nsright
  Querying then is very fast, insertions are not fast
- Closure table having another Table referencing each node 1 : m all children including the node itself
  Fast querying, fast updating, deletion, inserting, but needs a lot of space
** Chapter 4 - ID Required
Add new column like `id` as pseudo key or surrogate key to every table so that every row can be addressed uniquely
while also allowing all other attributes to contain duplicates

Objective: Establish primary key conventions

Problems addressed in this book:
- cargo cult that every table "needs" a single (maybe even autogenerated) primary key all looking the same and maybe all called `id`
- might make a redundant key
- allows duplicate rows
- obscures the meaning of a key
  - `id` is so generic that it holds no meaning
  - especially if query result is returned as dict (like in JSON) where multiple `id` would just silently overwrite
- writing joins won't work with USING `id` and joins get ugly and worse error prone to be written
- avoiding compound keys is just wrong:
  - like mathematician refusing to use 2d or 3d coordinates

Further problems IMHO:
- such ids are easy to misinterpret:
  - confound with other ids
  - think their ordering might matter
  - if there is a natural key column, it might be called similar and confuses the hell out of everyone
- makes it tough to migrate or roll back changes from tests, other branches, run distributed, etc
- uses extra space
- doesn't have a meaning especially not for the subject matter experts, but also not for the database experts per se
- messes up any hashing, e.g. inserting, deleting and inserting again should yield the same table,
  but with such an autogenerated surrogate key, it won't be the same row and if you track via a hash,
  it might even receive a different one (or you need complicate logic to find out whether the id is a real one or a surrogate one)
- similar: it's completely unclear whether the id is important in case of exports or even in contrast security problematic
- can't use it to feed into a machine learning (it's even worse and ML could be find a meaningless correlation and overfit)

Legitimate usages:
- pseudo key is a good choice as a surrogate for a natural key that's too long to be practical

Solution as in book:
- tell it like it is ::
  - sensible names for your primary key, like `bug_id` instead of `id`
  - *same* column name in foreign keys where possible (so joining via USING bug_id or similar will work like a charm)
  - name of primary key should be unique within your schema
- be unconventional :: change defaults for ORMs in case
- embrace natural keys and compound keys ::
  - if table contains an attribute that's guaranteed to be unique, non null and serve to identify the row => use it as primary key
  - use compound keys when they're appropriate

Further solutions IMHO:
- Hash the natural key candidates:
  - if it is unsure whether natural keys are guaranteed to stay natural keys in future
  - are too long
  - or have other problems, like they are sensitive and might not be exported or something like that, but you still need them for linking
** Chapter 5 - Keyless Entry (leaving out referential integrity to simplify database design/architecture/programming)
instead of fail early whenever a user submitted invalid data

Problems addressed in this book:
- assuming flawless code (in application logic) :: "One in a million is next Tuesday" (regarding low probabilities of errors)
- checking for mistakes :: via own written scripts having to run regulary (something I had to do a lot when working with data)
  is more effort than just setting the right constraints and rely on the database to do it
- "it's not my fault" :: your application might be fine, but other applications or database users or ad hoc SQL might not
  and again this will happen at some point in the lifetime of an application (IHMO, especially if it a successful application)
- Catch-22 updates :: to make updates and keep referential integrities, you need more than one database SQL statement
  shutting off foreign key enables it, but for the price of having inconsistent views in between

Further problems IMHO:
- makes it very hard to work with database exports or dumps
  as the important relationship between the data gets lost (and documentation in practice either is not accessible or outdated)
  I had to write more than once scripts to reverse engineer the foreign key relationships
  what is extremely difficult if the key ranges overlap and/or there are already inconsistencies in the database (as a result of no foreign key design)
  so while RDBMS should be very strict math oriented set theory things -> they become a non scientific guessing game
- it's difficulty to check data pipelines on consistency or subtle errors if the constraints are not hard enforced

Legitimate usages addressed in book:
- when doing huge data cleanup projects
- renaming database tables can be become nasty with foreign key definitions
  (author mentions pt-online-schema-change tool for MySQL)
  IMHO I'm sceptic about and personally would try to delegate the migration to a good tool (like Django ORM migrations or w/e)

Legitimate usages IMHO:
- when building highly dynamic ELT pipelines
  it might be easier and more performant to avoid creating foreign key relations
  Note: It's important not to do any modifications (that's why it's ok for ELT, but IMHO not for ETL)
  I think it's especially right, if the destination database is one that only exists for short time,
  like if you bring it into a pandas/SQLite in memory (read only) presentation

Solution:
- Declare constraints
- Define cascading updates
* Data Pipelines with Apache Airflow
** Chapter 1 - Meet Apache Airflow
- Data pipelines as graphs ::
  works on DAGs, so does not contain any loops or cycles
  extremely important, as it prevents us from running into circular dependencies
- Pipeline graphs vs sequential scripts ::
  single monolithic script may not initially seem like that much of a problem,
  but it can introduce some inefficiencies when tasks in the pipeline fail
- Defining pipelines flexibility in (python) code ::
  in airflow, define you DAGs using Python code in DAG files,
  which are essentially Python scripts that describe the structure of the corresponding DAG
- Reasons to choose Airflow ::
  features such as backfilling enable to easily (re)process historical data,
  allowing to recompute any derived data sets after making changes to your code
- Reasons not to choose Airflow ::
  - handling streaming pipelines
  - implementing highly dynamic pipelines
    although Airflow can implement this kind of dynamic behaviour,
    the web interface will only show tasks that are still defined in the most recent version of the DAG
  - it's primarly a workflow/pipeline management platform,
    does not contain features as maintaining data lineages, data versioning, ...
- Summary ::
  implementing efficient, batch-oriented data pipelines
** Chapter 2 - Anatomy of an Airflow DAG
- Running Airflow in a Python environment ::
  Make sure to install apache-airflow and not just airflow
- Summary ::
  - Workflows in Airflow are represented as DAGs
  - Operators represent a single unit of work
  - Airflow contains an array of operators both for generic and specific types of work
  - Airflow UI offers a graph view for viewing the DAG structure and tree view for viewing DAG runs over time
  - Failed tasks can be restarted anywhere in the DAG
